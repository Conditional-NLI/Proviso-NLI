{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:08.275877Z",
     "iopub.status.busy": "2025-08-01T01:24:08.275605Z",
     "iopub.status.idle": "2025-08-01T01:24:11.474847Z",
     "shell.execute_reply": "2025-08-01T01:24:11.473934Z",
     "shell.execute_reply.started": "2025-08-01T01:24:08.275861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:11.476964Z",
     "iopub.status.busy": "2025-08-01T01:24:11.476735Z",
     "iopub.status.idle": "2025-08-01T01:24:14.652603Z",
     "shell.execute_reply": "2025-08-01T01:24:14.651755Z",
     "shell.execute_reply.started": "2025-08-01T01:24:11.476944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:14.653906Z",
     "iopub.status.busy": "2025-08-01T01:24:14.653629Z",
     "iopub.status.idle": "2025-08-01T01:24:14.660605Z",
     "shell.execute_reply": "2025-08-01T01:24:14.659862Z",
     "shell.execute_reply.started": "2025-08-01T01:24:14.653882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import string\n",
    "\n",
    "from captum.attr import (\n",
    "    FeatureAblation, \n",
    "    ShapleyValues,\n",
    "    LayerIntegratedGradients, \n",
    "    LLMAttribution, \n",
    "    LLMGradientAttribution, \n",
    "    TextTokenInput, \n",
    "    TextTemplateInput,\n",
    "    ProductBaselines,\n",
    ")\n",
    "\n",
    "# Ignore warnings due to transformers library\n",
    "warnings.filterwarnings(\"ignore\", \".*past_key_values.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*Skipping this token.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:14.661899Z",
     "iopub.status.busy": "2025-08-01T01:24:14.661663Z",
     "iopub.status.idle": "2025-08-01T01:24:14.686132Z",
     "shell.execute_reply": "2025-08-01T01:24:14.685303Z",
     "shell.execute_reply.started": "2025-08-01T01:24:14.661883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers.models.gemma3.configuration_gemma3 import (\n",
    "    Gemma3Config, Gemma3TextConfig,\n",
    ")\n",
    "from transformers.models.gemma3.modeling_gemma3 import (\n",
    "    Gemma3TextModel, Gemma3PreTrainedModel,\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n",
    "\n",
    "\n",
    "# ───────────────────── helpers ─────────────────────────────────────────\n",
    "def _txt(cfg):                              # text sub-config\n",
    "    return getattr(cfg, \"text_config\", cfg)\n",
    "\n",
    "\n",
    "def _pool_last(token_logits, input_ids, pad_id):\n",
    "    if input_ids is None or pad_id is None:\n",
    "        return token_logits[:, -1]\n",
    "    ends = (~input_ids.eq(pad_id)).cumsum(-1).argmax(-1)\n",
    "    return token_logits[torch.arange(token_logits.size(0)), ends]\n",
    "\n",
    "\n",
    "def _compute_loss(config, logits, labels, num_labels):\n",
    "    if labels is None:\n",
    "        return None\n",
    "    if config.problem_type is None:\n",
    "        if num_labels == 1:\n",
    "            config.problem_type = \"regression\"\n",
    "        elif labels.dtype in (torch.long, torch.int):\n",
    "            config.problem_type = \"single_label_classification\"\n",
    "        else:\n",
    "            config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "    if config.problem_type == \"regression\":\n",
    "        return nn.MSELoss()(logits.squeeze(), labels.squeeze())\n",
    "    if config.problem_type == \"single_label_classification\":\n",
    "        return nn.CrossEntropyLoss()(logits, labels)\n",
    "    return nn.BCEWithLogitsLoss()(logits, labels)\n",
    "\n",
    "# ───────────────────── text-only variant (1 B) ────────────────────────\n",
    "class Gemma3TextForSequenceClassification(Gemma3PreTrainedModel):\n",
    "    \"\"\"\n",
    "    **Wraps** Gemma3TextModel in `self.model` – keeps `model.*`\n",
    "    prefixes so every pretrained weight loads.\n",
    "    \"\"\"\n",
    "    config_class = Gemma3TextConfig\n",
    "    keys_to_ignore_at_inference = [\"past_key_values\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.model = Gemma3TextModel(config)              # <- wrapper\n",
    "        self.score = nn.Linear(config.hidden_size,\n",
    "                               self.num_labels, bias=False)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None, attention_mask=None, position_ids=None,\n",
    "        inputs_embeds=None, past_key_values=None, labels=None,\n",
    "        use_cache=None, output_attentions=None,\n",
    "        output_hidden_states=None, return_dict=None,\n",
    "    ):\n",
    "        return_dict = (return_dict if return_dict is not None\n",
    "                       else self.config.use_return_dict)\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        token_logits = self.score(outputs.last_hidden_state)\n",
    "        pooled = _pool_last(token_logits, input_ids, self.config.pad_token_id)\n",
    "        loss = _compute_loss(self.config, pooled, labels, self.num_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            out = (pooled,) + outputs[1:]\n",
    "            return ((loss,) + out) if loss is not None else out\n",
    "\n",
    "        return SequenceClassifierOutputWithPast(\n",
    "            loss=loss, logits=pooled,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states\n",
    "                if output_hidden_states else None,\n",
    "            attentions=outputs.attentions\n",
    "                if output_attentions else None,\n",
    "        )\n",
    "\n",
    "\n",
    "# ───────────────────── register with HF factory ───────────────────────\n",
    "AutoModelForSequenceClassification.register(\n",
    "    Gemma3TextConfig, Gemma3TextForSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:14.688072Z",
     "iopub.status.busy": "2025-08-01T01:24:14.687892Z",
     "iopub.status.idle": "2025-08-01T01:24:14.706570Z",
     "shell.execute_reply": "2025-08-01T01:24:14.705925Z",
     "shell.execute_reply.started": "2025-08-01T01:24:14.688057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = \"10000MB\"\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        num_labels=3,\n",
    "        device_map=\"auto\",  \n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:14.707644Z",
     "iopub.status.busy": "2025-08-01T01:24:14.707380Z",
     "iopub.status.idle": "2025-08-01T01:24:14.866371Z",
     "shell.execute_reply": "2025-08-01T01:24:14.865592Z",
     "shell.execute_reply.started": "2025-08-01T01:24:14.707621Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(new_session=False, # Won’t request token if one is already saved on machine\n",
    "write_permission=True, # Requires a token with write permission\n",
    "token=\"\", # The name of your token\n",
    "add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:14.868154Z",
     "iopub.status.busy": "2025-08-01T01:24:14.867414Z",
     "iopub.status.idle": "2025-08-01T01:24:20.320313Z",
     "shell.execute_reply": "2025-08-01T01:24:20.319577Z",
     "shell.execute_reply.started": "2025-08-01T01:24:14.868130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"ConditionalNLI/exp8_gemma3\" \n",
    "bnb_config = create_bnb_config()\n",
    "model, tokenizer = load_model(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:20.321462Z",
     "iopub.status.busy": "2025-08-01T01:24:20.321245Z",
     "iopub.status.idle": "2025-08-01T01:24:20.375741Z",
     "shell.execute_reply": "2025-08-01T01:24:20.374982Z",
     "shell.execute_reply.started": "2025-08-01T01:24:20.321445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df4 = pd.read_excel('/kaggle/input/confer-extension/Part4B_Dataset/4B_Type4.xlsx')\n",
    "test_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:20.376854Z",
     "iopub.status.busy": "2025-08-01T01:24:20.376551Z",
     "iopub.status.idle": "2025-08-01T01:24:20.572218Z",
     "shell.execute_reply": "2025-08-01T01:24:20.571546Z",
     "shell.execute_reply.started": "2025-08-01T01:24:20.376834Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df5a = pd.read_excel('/kaggle/input/confer-extension/Part4B_Dataset/4B_Type5A.xlsx')\n",
    "test_df5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:20.573292Z",
     "iopub.status.busy": "2025-08-01T01:24:20.573015Z",
     "iopub.status.idle": "2025-08-01T01:24:20.669970Z",
     "shell.execute_reply": "2025-08-01T01:24:20.669277Z",
     "shell.execute_reply.started": "2025-08-01T01:24:20.573268Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df5p = pd.read_excel('/kaggle/input/confer-extension/Part4B_Dataset/4B_Type5P.xlsx')\n",
    "test_df5p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:20.670872Z",
     "iopub.status.busy": "2025-08-01T01:24:20.670647Z",
     "iopub.status.idle": "2025-08-01T01:24:20.678819Z",
     "shell.execute_reply": "2025-08-01T01:24:20.678191Z",
     "shell.execute_reply.started": "2025-08-01T01:24:20.670856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_test_df = test_df4.iloc[:4].reset_index(drop=True)\n",
    "sample_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:20.680241Z",
     "iopub.status.busy": "2025-08-01T01:24:20.679926Z",
     "iopub.status.idle": "2025-08-01T01:24:20.694242Z",
     "shell.execute_reply": "2025-08-01T01:24:20.693422Z",
     "shell.execute_reply.started": "2025-08-01T01:24:20.680216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test_df = sample_test_df\n",
    "test_df = test_df5p\n",
    "possessive_trigger = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:20.695351Z",
     "iopub.status.busy": "2025-08-01T01:24:20.695048Z",
     "iopub.status.idle": "2025-08-01T01:24:20.717276Z",
     "shell.execute_reply": "2025-08-01T01:24:20.716627Z",
     "shell.execute_reply.started": "2025-08-01T01:24:20.695330Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_ds = Dataset.from_pandas(test_df)\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:20.720233Z",
     "iopub.status.busy": "2025-08-01T01:24:20.719860Z",
     "iopub.status.idle": "2025-08-01T01:24:20.726309Z",
     "shell.execute_reply": "2025-08-01T01:24:20.725742Z",
     "shell.execute_reply.started": "2025-08-01T01:24:20.720216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "columns_to_keep = ['index', 'premise', 'hypothesis']\n",
    "test_ds = test_ds.remove_columns([col for col in test_ds.column_names if col not in columns_to_keep])\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:20.727084Z",
     "iopub.status.busy": "2025-08-01T01:24:20.726909Z",
     "iopub.status.idle": "2025-08-01T01:24:20.739802Z",
     "shell.execute_reply": "2025-08-01T01:24:20.738982Z",
     "shell.execute_reply.started": "2025-08-01T01:24:20.727071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"premise\"], example[\"hypothesis\"], truncation=True, max_length=256, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:20.740821Z",
     "iopub.status.busy": "2025-08-01T01:24:20.740557Z",
     "iopub.status.idle": "2025-08-01T01:24:21.279974Z",
     "shell.execute_reply": "2025-08-01T01:24:21.279244Z",
     "shell.execute_reply.started": "2025-08-01T01:24:20.740805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_test_ds = test_ds.map(tokenize_function, batched=True)\n",
    "tokenized_test_ds.set_format(\"torch\")\n",
    "tokenized_test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:21.281384Z",
     "iopub.status.busy": "2025-08-01T01:24:21.280853Z",
     "iopub.status.idle": "2025-08-01T01:24:21.287605Z",
     "shell.execute_reply": "2025-08-01T01:24:21.286910Z",
     "shell.execute_reply.started": "2025-08-01T01:24:21.281358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_test_ds = tokenized_test_ds.remove_columns([\"premise\", \"hypothesis\"])\n",
    "tokenized_test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:21.288633Z",
     "iopub.status.busy": "2025-08-01T01:24:21.288394Z",
     "iopub.status.idle": "2025-08-01T01:24:22.355362Z",
     "shell.execute_reply": "2025-08-01T01:24:22.354737Z",
     "shell.execute_reply.started": "2025-08-01T01:24:21.288607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Predicting Labels on Dataset\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
    "dataloader = DataLoader(tokenized_test_ds, batch_size=32, collate_fn=data_collator)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(preds.cpu().tolist())\n",
    "\n",
    "test_df['predicted_label'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:22.356267Z",
     "iopub.status.busy": "2025-08-01T01:24:22.356086Z",
     "iopub.status.idle": "2025-08-01T01:24:22.368326Z",
     "shell.execute_reply": "2025-08-01T01:24:22.367590Z",
     "shell.execute_reply.started": "2025-08-01T01:24:22.356253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df['expected_logits'] = test_df['gold_label'].map({'C':[1,0,0],'N':[0,1,0],'E':[0,0,1]})\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:22.370101Z",
     "iopub.status.busy": "2025-08-01T01:24:22.369199Z",
     "iopub.status.idle": "2025-08-01T01:24:22.384566Z",
     "shell.execute_reply": "2025-08-01T01:24:22.383902Z",
     "shell.execute_reply.started": "2025-08-01T01:24:22.370075Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogitWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = output.logits\n",
    "        logits = logits.unsqueeze(0) # add third dimension \n",
    "        output.logits = logits\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:22.385941Z",
     "iopub.status.busy": "2025-08-01T01:24:22.385393Z",
     "iopub.status.idle": "2025-08-01T01:24:22.402372Z",
     "shell.execute_reply": "2025-08-01T01:24:22.401613Z",
     "shell.execute_reply.started": "2025-08-01T01:24:22.385916Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wrapped_model = LogitWrapper(model)\n",
    "lig = LayerIntegratedGradients(wrapped_model, model.model.embed_tokens)\n",
    "llm_attr = LLMGradientAttribution(lig, tokenizer)\n",
    "skip_tokens = ['<bos>','<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:22.403507Z",
     "iopub.status.busy": "2025-08-01T01:24:22.403215Z",
     "iopub.status.idle": "2025-08-01T01:24:22.417718Z",
     "shell.execute_reply": "2025-08-01T01:24:22.416979Z",
     "shell.execute_reply.started": "2025-08-01T01:24:22.403487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_attrs(row):\n",
    "\n",
    "    premise = row['premise']\n",
    "    hypothesis = row['hypothesis']\n",
    "    target = torch.tensor(row['expected_logits'])    \n",
    "    \n",
    "    inp = TextTokenInput(\n",
    "        premise + tokenizer.eos_token + hypothesis,\n",
    "        tokenizer,\n",
    "        skip_tokens=skip_tokens,\n",
    "    )\n",
    "    attr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\n",
    "    result = {\n",
    "        'tokens': attr_res.input_tokens,\n",
    "        'attrs': attr_res.seq_attr\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:48:57.902598Z",
     "iopub.status.busy": "2025-08-01T01:48:57.902294Z",
     "iopub.status.idle": "2025-08-01T01:48:57.909633Z",
     "shell.execute_reply": "2025-08-01T01:48:57.908894Z",
     "shell.execute_reply.started": "2025-08-01T01:48:57.902576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def reformat_output(result):\n",
    "\n",
    "    i = 0 # position counter\n",
    "    token_gradients = defaultdict(float)\n",
    "    for token, grad in zip(result['tokens'], result['attrs']):\n",
    "        # if token.startswith('▁'):\n",
    "        #     token = token[1:]\n",
    "        \n",
    "        token_gradients[(token,i)] += grad.item()\n",
    "        i += 1 \n",
    "\n",
    "    token_gradients = dict(token_gradients)\n",
    "    tokens = [tok[0] for tok in list(token_gradients.keys())]\n",
    "    word_gradients = {}\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        word = tokens[i]\n",
    "        if word.startswith('▁'):\n",
    "            word = word[1:]\n",
    "        count = 1 \n",
    "        word_gradient = token_gradients[(token,i)]\n",
    "        if word in ['[SEP]','[CLS]']:\n",
    "             word_gradients[(word,i)] = word_gradient\n",
    "             continue # do not merge with next token\n",
    "        while (i+1)<len(tokens) and tokens[i+1] not in string.punctuation and not tokens[i+1].startswith('▁') and not tokens[i+1] == '[SEP]':\n",
    "            # part of same word, add gradients together and then average out \n",
    "            i += 1 # move to next word\n",
    "            word += tokens[i] # concatente to restore word \n",
    "            word_gradient += token_gradients[(tokens[i],i)]\n",
    "            count += 1\n",
    "        # end of word \n",
    "        word_gradients[(word,i)] = word_gradient/count # averaging\n",
    "    \n",
    "    return word_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:22.438094Z",
     "iopub.status.busy": "2025-08-01T01:24:22.437848Z",
     "iopub.status.idle": "2025-08-01T01:24:22.452404Z",
     "shell.execute_reply": "2025-08-01T01:24:22.451874Z",
     "shell.execute_reply.started": "2025-08-01T01:24:22.438070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_average_gradient(result):\n",
    "     # average gradient per token\n",
    "    return float(sum(result['attrs'])/len(result['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:24:22.453314Z",
     "iopub.status.busy": "2025-08-01T01:24:22.453122Z",
     "iopub.status.idle": "2025-08-01T01:24:22.467343Z",
     "shell.execute_reply": "2025-08-01T01:24:22.466793Z",
     "shell.execute_reply.started": "2025-08-01T01:24:22.453292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_trigger_gradients(row):\n",
    "    \n",
    "    gradients = 0\n",
    "    count = 0\n",
    "    trigger_words = []\n",
    "\n",
    "    # split phrases into individual words\n",
    "    trigger_words = row['key_phrase'].split()\n",
    "\n",
    "    # get gradient of all words in this example\n",
    "    word_gradients = row['word_gradients']\n",
    "    word_gradients_words = [key[0] for key in list(word_gradients.keys())] # extracting word portion of keys\n",
    "    \n",
    "    word_index_mapping = {}\n",
    "    \n",
    "    for word, index in list(word_gradients.keys()):\n",
    "        word_index_mapping.setdefault(word,index) # set the first value (premise value) as the index\n",
    "    \n",
    "    for word in trigger_words:\n",
    "        if word in word_gradients_words:\n",
    "            word_index = word_index_mapping[word]\n",
    "            gradients += word_gradients[(word,word_index)]\n",
    "            count += 1 \n",
    "        elif word.lower() in word_gradients_words:\n",
    "            word_index = word_index_mapping[word.lower()]\n",
    "            gradients += word_gradients[(word.lower(),word_index)]\n",
    "            count += 1 \n",
    "        else:\n",
    "            print(f\"{row.name}: {word} not found!\")\n",
    "\n",
    "    # compute avg gradient of trigger tokens\n",
    "    return gradients/count if count > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:28:41.101058Z",
     "iopub.status.busy": "2025-08-01T01:28:41.100349Z",
     "iopub.status.idle": "2025-08-01T01:28:41.107398Z",
     "shell.execute_reply": "2025-08-01T01:28:41.106625Z",
     "shell.execute_reply.started": "2025-08-01T01:28:41.101034Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_trigger_word_ranking(word_gradients,possessive_trigger):\n",
    "\n",
    "    sorted_dict = dict(sorted(word_gradients.items(), key=lambda item: item[1],reverse=True))\n",
    "    sorted_dict = {k: v for k,v in sorted_dict.items() if k[0] not in ['[CLS]','[SEP]'] and k[0] not in string.punctuation}\n",
    "    word_gradients_df = pd.DataFrame.from_dict(sorted_dict, orient='index', columns=['Gradient'])\n",
    "    word_gradients_df['Ranking'] = range(1, len(word_gradients_df) + 1)\n",
    "    word_gradients_df['Percentile Ranking'] = word_gradients_df['Ranking'].apply(lambda x: x / len(word_gradients_df))\n",
    "\n",
    "    word_gradients_words = [key[0] for key in list(word_gradients.keys())] # extracting word portion of keys \n",
    "\n",
    "    if possessive_trigger:\n",
    "        if 'his' in word_gradients_words:\n",
    "            key_trigger_word = 'his'\n",
    "        elif 'her' in word_gradients_words:\n",
    "            key_trigger_word = 'her'\n",
    "        elif 'their' in word_gradients_words:\n",
    "            key_trigger_word = 'their'\n",
    "    else:\n",
    "        key_trigger_word = 'again'\n",
    "\n",
    "    try:\n",
    "        key_trigger_word_rank = word_gradients_df.loc[key_trigger_word]['Ranking']\n",
    "        key_trigger_word_percent_rank = word_gradients_df.loc[key_trigger_word]['Percentile Ranking']\n",
    "    except:\n",
    "        key_trigger_word_rank = None \n",
    "        key_trigger_word_percent_rank = None\n",
    "        print(f\"Trigger word not found! - {key_trigger_word}\")\n",
    "\n",
    "    return pd.Series([key_trigger_word_rank, key_trigger_word_percent_rank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:47:35.426049Z",
     "iopub.status.busy": "2025-08-01T01:47:35.425764Z",
     "iopub.status.idle": "2025-08-01T01:47:46.633113Z",
     "shell.execute_reply": "2025-08-01T01:47:46.632472Z",
     "shell.execute_reply.started": "2025-08-01T01:47:35.426029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df['attr_result'] = test_df.apply(compute_attrs, axis=1)\n",
    "test_df.to_csv('part4b_type5p_gemma_raw.csv')\n",
    "test_df['word_gradients'] = test_df['attr_result'].apply(reformat_output)\n",
    "test_df['average_gradient'] = test_df['attr_result'].apply(get_average_gradient)\n",
    "test_df['trigger_gradient'] = test_df.apply(compute_trigger_gradients,axis=1)\n",
    "test_df['predicted_label'] = test_df['predicted_label'].map({0:'C',1:'N',2:'E'})\n",
    "test_df[['trigger_word_ranking','trigger_word_percentile_ranking']] = test_df['word_gradients'].apply(compute_trigger_word_ranking,args=(possessive_trigger,))\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T01:47:56.262207Z",
     "iopub.status.busy": "2025-08-01T01:47:56.261493Z",
     "iopub.status.idle": "2025-08-01T01:47:56.267566Z",
     "shell.execute_reply": "2025-08-01T01:47:56.266904Z",
     "shell.execute_reply.started": "2025-08-01T01:47:56.262182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df['word_gradients'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T17:59:50.345072Z",
     "iopub.status.busy": "2025-06-21T17:59:50.344417Z",
     "iopub.status.idle": "2025-06-21T17:59:50.360673Z",
     "shell.execute_reply": "2025-06-21T17:59:50.360022Z",
     "shell.execute_reply.started": "2025-06-21T17:59:50.345043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('part4b_type5p_gemma_results.csv') "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7663069,
     "sourceId": 12315798,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

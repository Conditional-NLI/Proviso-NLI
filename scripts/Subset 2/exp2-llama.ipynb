{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.167Z",
     "iopub.execute_input": "2025-08-08T01:02:13.085242Z",
     "iopub.status.busy": "2025-08-08T01:02:13.084983Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.5.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m00:01\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import string\n",
    "\n",
    "from captum.attr import (\n",
    "    FeatureAblation, \n",
    "    ShapleyValues,\n",
    "    LayerIntegratedGradients, \n",
    "    LLMAttribution, \n",
    "    LLMGradientAttribution, \n",
    "    TextTokenInput, \n",
    "    TextTemplateInput,\n",
    "    ProductBaselines,\n",
    ")\n",
    "\n",
    "# Ignore warnings due to transformers library\n",
    "warnings.filterwarnings(\"ignore\", \".*past_key_values.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*Skipping this token.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = \"10000MB\"\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        num_labels=3,\n",
    "        device_map=\"auto\",  # dispatch efficiently the model on the available ressources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(new_session=False, # Won’t request token if one is already saved on machine\n",
    "write_permission=True, # Requires a token with write permission\n",
    "token=\"\", # The name of your token\n",
    "add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"ConditionalNLI/exp8_llama3.2\" \n",
    "bnb_config = create_bnb_config()\n",
    "model, tokenizer = load_model(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.problem_type = \"single_label_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df4 = pd.read_csv('/kaggle/input/confer-extension/Part4A_Dataset/part4a_type4_final.csv',index_col=0)\n",
    "test_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df5a = pd.read_csv('/kaggle/input/confer-extension/Part4A_Dataset/part4a_type5a_final.csv',index_col=0)\n",
    "test_df5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df5p = pd.read_csv('/kaggle/input/confer-extension/Part4A_Dataset/part4a_type5p_final.csv',index_col=0)\n",
    "test_df5p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_test_df = test_df4.iloc[:4].reset_index(drop=True)\n",
    "sample_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df = sample_test_df\n",
    "# test_df = test_df5p\n",
    "possessive_trigger = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_ds = Dataset.from_pandas(test_df)\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "columns_to_keep = ['index', 'premise', 'hypothesis']\n",
    "test_ds = test_ds.remove_columns([col for col in test_ds.column_names if col not in columns_to_keep])\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"premise\"], example[\"hypothesis\"], truncation=True, max_length=256, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_test_ds = test_ds.map(tokenize_function, batched=True)\n",
    "tokenized_test_ds.set_format(\"torch\")\n",
    "tokenized_test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_test_ds = tokenized_test_ds.remove_columns([\"premise\", \"hypothesis\"])\n",
    "tokenized_test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Predicting Labels on Dataset\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
    "dataloader = DataLoader(tokenized_test_ds, batch_size=32, collate_fn=data_collator)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(preds.cpu().tolist())\n",
    "\n",
    "test_df['predicted_label'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df['expected_logits'] = test_df['gold_label'].map({'C':[1,0,0],'N':[0,1,0],'E':[0,0,1]})\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogitWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = output.logits\n",
    "        logits = logits.unsqueeze(0) # add third dimension \n",
    "        output.logits = logits\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wrapped_model = LogitWrapper(model)\n",
    "lig = LayerIntegratedGradients(wrapped_model, model.model.embed_tokens)\n",
    "llm_attr = LLMGradientAttribution(lig, tokenizer)\n",
    "skip_tokens = ['<|begin_of_text|>','<|end_of_text|>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_attrs(row):\n",
    "\n",
    "    premise = row['premise']\n",
    "    hypothesis = row['hypothesis']\n",
    "    target = torch.tensor(row['expected_logits'])    \n",
    "    \n",
    "    inp = TextTokenInput(\n",
    "        premise + tokenizer.eos_token + hypothesis,\n",
    "        tokenizer,\n",
    "        skip_tokens=skip_tokens,\n",
    "    )\n",
    "    attr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\n",
    "    result = {\n",
    "        'tokens': attr_res.input_tokens,\n",
    "        'attrs': attr_res.seq_attr\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def reformat_output(result):\n",
    "\n",
    "    i = 0 # position counter\n",
    "    token_gradients = defaultdict(float)\n",
    "    for token, grad in zip(result['tokens'], result['attrs']):\n",
    "        if token.startswith('▁'):\n",
    "            token = token[1:]\n",
    "        \n",
    "        token_gradients[(token,i)] += grad.item()\n",
    "        i += 1 \n",
    "\n",
    "    token_gradients = dict(token_gradients)\n",
    "    tokens = [tok[0] for tok in list(token_gradients.keys())]\n",
    "    word_gradients = {}\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        word = tokens[i]\n",
    "        if word.startswith('Ġ'):\n",
    "            word = word[1:]\n",
    "        count = 1 \n",
    "        word_gradient = token_gradients[(token,i)]\n",
    "        if word in ['[SEP]','[CLS]']:\n",
    "             word_gradients[(word,i)] = word_gradient\n",
    "             continue # do not merge with next token\n",
    "        while (i+1)<len(tokens) and tokens[i+1] not in string.punctuation and not tokens[i+1].startswith('Ġ') and not tokens[i+1] == '[SEP]':\n",
    "            # part of same word, add gradients together and then average out \n",
    "            i += 1 # move to next word\n",
    "            word += tokens[i] # concatente to restore word \n",
    "            word_gradient += token_gradients[(tokens[i],i)]\n",
    "            count += 1\n",
    "        # end of word \n",
    "        word_gradients[(word,i)] = word_gradient/count # averaging\n",
    "    \n",
    "    return word_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_average_gradient(result):\n",
    "     # average gradient per token\n",
    "    return float(sum(result['attrs'])/len(result['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_trigger_gradients(row):\n",
    "    \n",
    "    gradients = 0\n",
    "    count = 0\n",
    "    trigger_words = []\n",
    "\n",
    "    # split phrases into individual words\n",
    "    trigger_words = row['Trigger Words'].split()\n",
    "\n",
    "    # get gradient of all words in this example\n",
    "    word_gradients = row['word_gradients']\n",
    "    word_gradients_words = [key[0] for key in list(word_gradients.keys())] # extracting word portion of keys\n",
    "    \n",
    "    word_index_mapping = {}\n",
    "    \n",
    "    for word, index in list(word_gradients.keys()):\n",
    "        word_index_mapping.setdefault(word,index) # set the first value (premise value) as the index\n",
    "    \n",
    "    for word in trigger_words:\n",
    "        if word in word_gradients_words:\n",
    "            word_index = word_index_mapping[word]\n",
    "            gradients += word_gradients[(word,word_index)]\n",
    "            count += 1 \n",
    "        elif word.lower() in word_gradients_words:\n",
    "            word_index = word_index_mapping[word.lower()]\n",
    "            gradients += word_gradients[(word.lower(),word_index)]\n",
    "            count += 1 \n",
    "        else:\n",
    "            print(f\"{row.name}: {word} not found!\")\n",
    "\n",
    "    # compute avg gradient of trigger tokens\n",
    "    return gradients/count if count > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_trigger_word_ranking(word_gradients,possessive_trigger):\n",
    "\n",
    "    sorted_dict = dict(sorted(word_gradients.items(), key=lambda item: item[1],reverse=True))\n",
    "    sorted_dict = {k: v for k,v in sorted_dict.items() if k[0] not in ['[CLS]','[SEP]'] and k[0] not in string.punctuation}\n",
    "    word_gradients_df = pd.DataFrame.from_dict(sorted_dict, orient='index', columns=['Gradient'])\n",
    "    word_gradients_df.index = pd.MultiIndex.from_tuples(word_gradients_df.index, names=['Word', 'Position'])\n",
    "    word_gradients_df['Ranking'] = range(1, len(word_gradients_df) + 1)\n",
    "    word_gradients_df['Percentile Ranking'] = word_gradients_df['Ranking'].apply(lambda x: x / len(word_gradients_df))\n",
    "\n",
    "    word_gradients_words = [key[0] for key in list(word_gradients.keys())] # extracting word portion of keys \n",
    "    word_index_mapping = {}\n",
    "    for word, index in list(word_gradients.keys()):\n",
    "        word_index_mapping.setdefault(word,index) # set the first value (premise value) as the index\n",
    "\n",
    "    key_trigger_word = ''\n",
    "    if possessive_trigger:\n",
    "        if 'his' in word_gradients_words:\n",
    "            key_trigger_word = 'his'\n",
    "        elif 'her' in word_gradients_words:\n",
    "            key_trigger_word = 'her'\n",
    "        elif 'their' in word_gradients_words:\n",
    "            key_trigger_word = 'their'\n",
    "    else:\n",
    "        key_trigger_word = 'again'\n",
    "\n",
    "    try:\n",
    "        key_trigger_word_index = word_index_mapping[key_trigger_word]\n",
    "        key_trigger_word_key = (f'{key_trigger_word}', key_trigger_word_index)    \n",
    "        key_trigger_word_rank = word_gradients_df.loc[key_trigger_word_key]['Ranking']\n",
    "        key_trigger_word_percent_rank = word_gradients_df.loc[key_trigger_word_key]['Percentile Ranking']\n",
    "    except:\n",
    "        key_trigger_word_rank = None \n",
    "        key_trigger_word_percent_rank = None\n",
    "        print(f\"Trigger word not found! - {key_trigger_word}\")\n",
    "\n",
    "    return pd.Series([key_trigger_word_rank, key_trigger_word_percent_rank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df['attr_result'] = test_df.apply(compute_attrs, axis=1)\n",
    "test_df.to_csv('part4a_type5p_llama_raw.csv')\n",
    "test_df['word_gradients'] = test_df['attr_result'].apply(reformat_output)\n",
    "test_df['average_gradient'] = test_df['attr_result'].apply(get_average_gradient)\n",
    "test_df['trigger_gradient'] = test_df.apply(compute_trigger_gradients,axis=1)\n",
    "test_df.to_csv('part4a_type5p_llama_results_new.csv')\n",
    "test_df['predicted_label'] = test_df['predicted_label'].map({0:'C',1:'N',2:'E'})\n",
    "test_df[['trigger_word_ranking','trigger_word_percentile_ranking']] = test_df['word_gradients'].apply(compute_trigger_word_ranking,args=(possessive_trigger,))\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-08T01:02:42.170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('part4a_type5p_llama_results_new.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6611530,
     "sourceId": 10674614,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6568489,
     "sourceId": 12019260,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7663069,
     "sourceId": 12315798,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
